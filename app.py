import streamlit as st
import pandas as pd
from urllib.parse import quote_plus
import numpy as np
import html
import requests
import io

st.set_page_config(
    page_title="ğŸ“˜ ãã£ã¨ã‚ˆã‚Šãã†æœ¬ã•ãŒã—",
    page_icon="ğŸ“˜",
    layout="centered",
    initial_sidebar_state="collapsed",
)

st.markdown(
    """
    <style>
    /* Base spacing */
    .main .block-container { padding-top: 1.5rem; padding-bottom: 2rem; }

    /* Headings readability */
    h1, h2, h3 { line-height: 1.3; }

    /* Make the main content column comfortably narrow on large screens */
    @media (min-width: 1024px) {
      .main .block-container { max-width: 940px; }
    }

    /* Mobile tweaks */
    @media (max-width: 640px) {
      .stButton>button { width: 100%; }
      .stRadio, .stSelectbox { font-size: 0.95rem; }
      .stMarkdown p { font-size: 0.98rem; line-height: 1.7; }
    }

    /* Cards & grid for desktop/mobile */
    .hero-card, .book-card {
      background: var(--card-bg, #fff);
      border: 1px solid var(--card-border, #e6e6e6);
      border-radius: 12px;
      padding: 16px 18px;
      box-shadow: 0 1px 2px rgba(0,0,0,0.04);
    }
    .hero-card { border-width: 2px; }
    .hero-title, .book-title { font-weight: 700; margin-bottom: 6px; font-size: 1.12rem; line-height: 1.5; }
    .hero-desc, .book-desc { margin: 8px 0 12px; line-height: 1.7; overflow-wrap: anywhere; }

    .book-grid { display: grid; gap: 12px; }
    @media (min-width: 768px) { .book-grid { grid-template-columns: 1fr 1fr; } }

    /* Link button */
    .link-btn { display:inline-block; padding: 9px 14px; border-radius: 8px; text-decoration:none; border:1px solid #ddd; background:#eef2ff; color:#1d4ed8; border-color:#c7d2fe; font-size:1rem; }
    .link-btn:hover { background:#e0e7ff; }

    /* Dark mode friendly defaults */
    @media (prefers-color-scheme: dark) {
      :root { --card-bg: #111418; --card-border: #242a31; }
      .link-btn { border-color:#333; }
      .link-btn:hover { background:#1c222b; }
    }

    /* Typography & base */
    html, body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji"; }
    body { -webkit-font-smoothing: antialiased; }

    /* Section spacing */
    .section { margin: 24px 0; }

    /* Improve label wrapping */
    .stSelectbox label, .stRadio label { white-space: normal; line-height: 1.5; }

    /* Clamp long descriptions on mobile */
    @media (max-width: 640px) {
      .book-desc { display: -webkit-box; -webkit-line-clamp: 5; -webkit-box-orient: vertical; overflow: hidden; }
    }

    /* details/summary styling */
    details { margin-top: 6px; }
    details summary { cursor: pointer; color: #3366cc; outline: none; }
    details[open] summary { color: #555; }

    /* Larger touch targets on mobile */
    @media (max-width: 640px) {
      .link-btn { width: 100%; text-align: center; padding: 12px 14px; }
    }

    /* Sections / dividers */
    .section { margin: 24px 0; }
    .section-hero { margin-bottom: 28px; }
    .section-others { margin-top: 14px; }
    .divider { height: 1px; background: var(--card-border, #e6e6e6); margin: 14px 0; }

    /* è¦‹å‡ºã—ã‚µã‚¤ã‚ºã‚’å°‘ã—æŠ‘ãˆã‚‹ */
    h1 { font-size: 1.4rem !important; }
    h2 { font-size: 1.15rem !important; }

    /* æœ¬æ–‡ã‚„èª¬æ˜ã¯ã»ã‚“ã®å°‘ã—å¤§ãã */
    .stMarkdown p, .book-desc {
      font-size: 1.02rem;
      line-height: 1.65;
    }

    /* Headings spacing/weight (æ§ãˆã‚ã«) */
    h1 { margin: 0.2rem 0 1.0rem; font-weight: 700; }
    h2 { margin: 0.8rem 0 0.6rem; font-weight: 650; }

    /* Success alert: å°‘ã—æ§ãˆã‚ã« */
    .stAlert { border-radius: 10px; margin: 8px 0 10px; }
    .stAlert > div { padding: 8px 10px; font-size: 0.9rem; }

    /* Radio/Select ã®è¡Œé–“ã¨ã‚¿ãƒƒãƒ—ã—ã‚„ã™ã• */
    .stRadio label, .stSelectbox label { margin-bottom: 4px; }
    .stRadio [role="radiogroup"] label { padding: 4px 2px; }

    /* Amazonãƒªãƒ³ã‚¯ã‚’ãƒœã‚¿ãƒ³é¢¨ã«ï¼ˆè‰²å¼±ã§ã‚‚è¦‹ã‚„ã™ã„ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆï¼‰ */
    .link-btn { background:#eef2ff; color:#1d4ed8; border-color:#c7d2fe; font-size:1rem; padding:9px 14px; }
    .link-btn:hover { background:#e0e7ff; }

    /* ã‚«ãƒ¼ãƒ‰é–“ã®ä½™ç™½ã‚’ã‚„ã‚„åºƒãï¼ˆPCï¼‰ */
    @media (min-width: 1024px) {
      .book-grid { gap: 16px; }
    }

    /* Desktop typography boost */
    @media (min-width: 768px) {
      h1 { font-size: 1.6rem !important; }
      h2 { font-size: 1.25rem !important; }
      .stMarkdown p, .book-desc { font-size: 1.05rem; line-height: 1.7; }
      .stRadio, .stSelectbox { font-size: 1rem; }
      .hero-title { font-size: 1.1rem; }
      .book-title { font-size: 1.05rem; }
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¤ã & ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
@st.cache_data(show_spinner=False, ttl=60 * 10)
def load_books() -> pd.DataFrame:
    url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vRrOkycGi4nVcR_f2HES6pkm4Yz8BiwFr2L9t3Zf0_j0c_eRy0g2pM9cxZj6fRfsUM20urikULvOqub/pub?output=csv"
    headers = {"User-Agent": "Mozilla/5.0 (compatible; matsuda-book-app-v1/1.0)"}
    text = None
    # Try up to 3 times with backoff
    for i in range(3):
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            resp.raise_for_status()
            try:
                text = resp.content.decode("utf-8-sig", errors="replace")
            except Exception:
                text = resp.content.decode("cp932", errors="replace")
            break
        except Exception:
            pass
    if text is None:
        st.error("ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        return pd.DataFrame(columns=["title", "description", "amazon_url", "keywords"])  # safe empty

    df = pd.read_csv(io.StringIO(text))
    # æ¨™æº–åŒ–
    df.columns = df.columns.str.strip().str.replace('"', "").str.replace("\n", "", regex=False)
    # æœŸå¾…ã‚«ãƒ©ãƒ åã¸ã®å¯„ã›
    rename_map: dict[str, str] = {}
    for c in df.columns:
        lc = c.lower()
        if "title" in lc and " " not in lc:
            rename_map[c] = "title"
        if "amazon" in lc:
            rename_map[c] = "amazon_url"
        if "keyword" in lc:
            rename_map[c] = "keywords"
        if "description" in lc:
            rename_map[c] = "description"
    df = df.rename(columns=rename_map)
    # æ¬ æã‚«ãƒ©ãƒ ã®å®‰å…¨å¯¾ç­–
    for col in ["title", "description", "amazon_url", "keywords"]:
        if col not in df.columns:
            df[col] = ""
    # å‰å¾Œç©ºç™½ã®é™¤å»
    for col in ["title", "description", "amazon_url", "keywords"]:
        df[col] = df[col].astype(str).str.strip()
    # é‡è¤‡æ’é™¤ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã§ä¸€æ„ã«ï¼‰
    if "title" in df.columns:
        df = df.drop_duplicates(subset=["title"]).reset_index(drop=True)
    return df

books = load_books()

st.title("ğŸ“˜ ä»Šæ—¥ã®ã‚ãªãŸã«ã€ãã£ã¨ã‚ˆã‚Šãã†æœ¬ã‚’æ¢ã—ã¾ã—ã‚‡ã†")

# è³ªå•
interest = st.selectbox("ãƒ†ãƒ¼ãƒã‚’é¸ã‚“ã§ãã ã•ã„", (
    "è‡ªå·±ç†è§£ãƒ»å†…çœ",
    "ç¿’æ…£ãƒ»ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«",
    "ä»•äº‹ãƒ»ã‚­ãƒ£ãƒªã‚¢",
    "äººé–“é–¢ä¿‚ãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
    "æ‹æ„›ãƒ»ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—",
    "å­è‚²ã¦ãƒ»æ•™è‚²",
    "æ­»ç”Ÿè¦³ãƒ»äººç”Ÿã®æ„å‘³"
))

feeling = st.radio("ä»Šã®æ°—æŒã¡ã«è¿‘ã„ã‚‚ã®ã‚’æ•™ãˆã¦ãã ã•ã„", (
    "å‰å‘ãã«ãªã‚ŠãŸã„",
    "è¿·ã„ã‚’æ•´ç†ã—ãŸã„",
    "è‡ªåˆ†ã®è»¸ã‚’ç¢ºã‹ã‚ãŸã„",
    "äººé–“é–¢ä¿‚ã‚’æ•´ãˆãŸã„",
    "å°ã•ãå‹•ãå‡ºã—ãŸã„",
))

# è¿½åŠ ã®ä¸€å•ï¼ˆèª­ã¿æ–¹/ã‚¢ãƒ—ãƒ­ãƒ¼ãƒè»¸ï¼‰ï¼šç²¾åº¦ã‚’å°‘ã—é«˜ã‚ã‚‹
extra = st.radio(
    "ä»Šå›ã¯ã©ã‚“ãªèª­ã¿æ–¹ãŒã—ã£ãã‚Šãã¾ã™ã‹ï¼Ÿ",
    (
        "ã•ã‚‰ã£ã¨èª­ã¿ãŸã„",
        "ã˜ã£ãã‚Šè€ƒãˆãŸã„",
        "å…·ä½“çš„ã«å®Ÿè·µã—ãŸã„",
    ),
)

# --- æ¨è–¦ãƒ­ã‚¸ãƒƒã‚¯ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸ ---
INTEREST_TO_KEYWORDS = {
    "è‡ªå·±ç†è§£ãƒ»å†…çœ": ["è‡ªå·±ç†è§£", "è‡ªåˆ†æ¢ã—", "å†…çœ", "ä¾¡å€¤è¦³", "è‡ªåˆ†è»¸", "å•ã„", "è³ªå•"],
    "ç¿’æ…£ãƒ»ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«": ["ç¿’æ…£", "ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³", "æ™‚é–“è¡“", "ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«", "æœæ´»", "æ‰‹æ”¾ã™"],
    "ä»•äº‹ãƒ»ã‚­ãƒ£ãƒªã‚¢": ["ä»•äº‹", "ã‚­ãƒ£ãƒªã‚¢", "ãƒ“ã‚¸ãƒã‚¹", "ä»•äº‹è¡“", "ä¼šç¤¾", "ä¸Šå¸", "èµ·æ¥­"],
    "äººé–“é–¢ä¿‚ãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³": ["äººé–“é–¢ä¿‚", "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "èã", "ä¼šè©±", "å‚¾è´", "è³ªå•åŠ›"],
    "æ‹æ„›ãƒ»ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—": ["æ‹æ„›", "ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼", "å¤«å©¦", "é–¢ä¿‚", "æ„›"],
    "å­è‚²ã¦ãƒ»æ•™è‚²": ["å­è‚²ã¦", "æ•™è‚²", "è¦ªå­", "å…ˆç”Ÿ", "å­¦æ ¡", "å­ã©ã‚‚"],
    "æ­»ç”Ÿè¦³ãƒ»äººç”Ÿã®æ„å‘³": ["æ­»", "äººç”Ÿã®æ„å‘³", "ç”Ÿãæ–¹", "å¹¸ç¦", "å“²å­¦"],
}

FEELING_TO_KEYWORDS = {
    "å‰å‘ãã«ãªã‚ŠãŸã„": ["ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³", "å‰å‘ã", "å…ƒæ°—", "æƒ…ç†±", "ã‚„ã‚‹æ°—", "ãƒã‚¸ãƒ†ã‚£ãƒ–", "å‹‡æ°—", "ã“ã“ã‚"],
    "è¿·ã„ã‚’æ•´ç†ã—ãŸã„": ["ãƒ¢ãƒ¤ãƒ¢ãƒ¤", "æ‚©ã¿", "æ•´ç†", "æ‰‹æ”¾ã™", "ã‚„ã‚ã‚‹", "æ–­æ¨é›¢", "ä¸å®‰", "æ•´ç†ã™ã‚‹"],
    "è‡ªåˆ†ã®è»¸ã‚’ç¢ºã‹ã‚ãŸã„": ["è‡ªåˆ†è»¸", "ä¾¡å€¤è¦³", "å¼·ã¿", "ç†æƒ³"],
    "äººé–“é–¢ä¿‚ã‚’æ•´ãˆãŸã„": ["äººé–“é–¢ä¿‚", "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "å‚¾è´", "ä¼šè©±", "ä¸Šå¸", "éƒ¨ä¸‹", "ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼", "æ‹æ„›"],
    "å°ã•ãå‹•ãå‡ºã—ãŸã„": ["ä¸€æ­©", "èƒŒä¸­ã‚’æŠ¼ã™", "ãã£ã‹ã‘", "ã‚¹ãƒ¢ãƒ¼ãƒ«ã‚¹ãƒ†ãƒƒãƒ—"],
}

EXTRA_TO_KEYWORDS = {
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ/èª­ã¿å£ã‚’ç¤ºã™èªã«å¯„ã›ã‚‹ï¼ˆãƒ†ãƒ¼ãƒèªã¨ã¯è¢«ã‚‰ãªã„ï¼‰
    "ã•ã‚‰ã£ã¨èª­ã¿ãŸã„": [
        "çŸ­ç·¨", "ã‚³ãƒ©ãƒ ", "è¦ç‚¹", "ã¾ã¨ã‚", "Q&A", "ç®‡æ¡æ›¸ã", "å›³è§£", "è¦‹é–‹ã", "ãƒãƒ³ã‚¬", "èª­ã¿ã‚„ã™ã„"
    ],
    "ã˜ã£ãã‚Šè€ƒãˆãŸã„": [
        "è«–è€ƒ", "è§£èª¬", "é•·æ–‡", "è€ƒå¯Ÿ", "èª­ã¿å¿œãˆ", "ç« æœ«", "å•ã„ã‹ã‘", "ã‚¨ãƒƒã‚»ã‚¤"
    ],
    "å…·ä½“çš„ã«å®Ÿè·µã—ãŸã„": [
        "ãƒ¯ãƒ¼ã‚¯", "æ¼”ç¿’", "ã‚·ãƒ¼ãƒˆ", "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ", "ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ", "ã‚¹ãƒ†ãƒƒãƒ—", "æ‰‹é †", "å®Ÿè·µã‚¬ã‚¤ãƒ‰"
    ],
}

INTEREST_PENALTY = {
    "è‡ªå·±ç†è§£ãƒ»å†…çœ": ["æ‹æ„›", "ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼", "å¤«å©¦", "å­è‚²ã¦", "æ•™è‚²", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "èµ·æ¥­", "ãƒ“ã‚¸ãƒã‚¹"],
    "ç¿’æ…£ãƒ»ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«": ["æ‹æ„›", "å­è‚²ã¦", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "èµ·æ¥­"],
    "ä»•äº‹ãƒ»ã‚­ãƒ£ãƒªã‚¢": ["æ‹æ„›", "å­è‚²ã¦", "æ­»", "æ­»ç”Ÿè¦³"],
    "äººé–“é–¢ä¿‚ãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³": ["èµ·æ¥­", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "ãƒãƒ³ãƒ€ãƒ©", "æ™‚é–“è¡“"],
    "æ‹æ„›ãƒ»ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—": ["èµ·æ¥­", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "ä»•äº‹è¡“"],
    "å­è‚²ã¦ãƒ»æ•™è‚²": ["æ‹æ„›", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "èµ·æ¥­"],
    "æ­»ç”Ÿè¦³ãƒ»äººç”Ÿã®æ„å‘³": ["ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "èµ·æ¥­", "æ‹æ„›", "å­è‚²ã¦"],
}

# Amazonæ¤œç´¢ãƒªãƒ³ã‚¯ã®ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒˆãƒ« + è‘—è€…ï¼‰
AUTHOR_TOKENS = ["ãƒãƒ„ãƒ€ãƒŸãƒ’ãƒ­", "æ¾ç”°å……å¼˜", "WAKANA"]

def guess_author_from_keywords(kw: str) -> str | None:
    if not isinstance(kw, str):
        return None
    if any(tok in kw for tok in ["WAKANA", "ãƒ¯ã‚«ãƒŠ", "ã‚ã‹ãª"]):
        return "WAKANA"
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒãƒ„ãƒ€ãƒŸãƒ’ãƒ­ç³»
    return "ãƒãƒ„ãƒ€ãƒŸãƒ’ãƒ­"


def build_amazon_link(title: str, author: str | None = None) -> str:
    """Amazonã®æ¤œç´¢URLã‚’ä½œã‚‹ï¼ˆç›´æ¥URLã¯ä½¿ã‚ãšæ¤œç´¢ãƒšãƒ¼ã‚¸ã«çµ±ä¸€ï¼‰ã€‚"""
    if author and isinstance(author, str):
        query = quote_plus(f"{title} {author}")
    else:
        # è‘—è€…ä¸æ˜ãªã‚‰æ—¢çŸ¥ã®è‘—è€…å€™è£œã‚’å«ã‚ã¦æ¤œç´¢
        query = quote_plus(f"{title} " + " OR ".join(AUTHOR_TOKENS))
    return f"https://www.amazon.co.jp/s?k={query}"

def _match_any_keyword(cell: str, kw_list: list[str]) -> bool:
    if not isinstance(cell, str):
        return False
    text = cell.lower()
    return any(k.lower() in text for k in kw_list)

# Helper: normalize and concatenate text parts
def _normalize_text(*parts: str) -> str:
    return "\n".join([(p or "").lower().strip() for p in parts])

def filter_books(df: pd.DataFrame, interest_choice: str, feeling_choice: str, extra_choice: str = "") -> pd.DataFrame:
    # 1) ãƒ†ãƒ¼ãƒãƒ»æ°—æŒã¡ ä¸¡æ–¹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’çµåˆ
    interest_kw = INTEREST_TO_KEYWORDS.get(interest_choice, [])
    feeling_kw = FEELING_TO_KEYWORDS.get(feeling_choice, [])
    combined_kw = list(dict.fromkeys(interest_kw + feeling_kw))  # é‡è¤‡é™¤å»ãƒ»é †åºç¶­æŒ

    penalty_kw = INTEREST_PENALTY.get(interest_choice, [])
    extra_kw = EXTRA_TO_KEYWORDS.get(extra_choice, [])

    # 2) ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¬„ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å…¨ä»¶
    if "keywords" not in df.columns:
        return df.copy()

    # 3) Weighted scoring using title/description/keywords
    df = df.copy()
    # precompute lowercase fields
    df["_title"] = df["title"].astype(str).str.lower()
    df["_desc"]  = df["description"].astype(str).str.lower()
    df["_keys"]  = df["keywords"].astype(str).str.lower()

    # boolean matches
    df["mi_keys"] = df["_keys"].apply(lambda s: _match_any_keyword(s, interest_kw))
    df["mi_title"] = df["_title"].apply(lambda s: _match_any_keyword(s, interest_kw))
    df["mi_desc"] = df["_desc"].apply(lambda s: _match_any_keyword(s, interest_kw))

    df["mf_keys"] = df["_keys"].apply(lambda s: _match_any_keyword(s, feeling_kw))
    df["mf_desc"] = df["_desc"].apply(lambda s: _match_any_keyword(s, feeling_kw))

    df["mx_keys"] = df["_keys"].apply(lambda s: _match_any_keyword(s, extra_kw))

    df["penalty"] = df["_keys"].apply(lambda s: _match_any_keyword(s, penalty_kw))

    # weighted score
    df["score"] = (
        (df["mi_keys"].astype(int) * 3)
        + (df["mi_title"].astype(int) * 2)
        + (df["mi_desc"].astype(int) * 1)
        + (df["mf_keys"].astype(int) * 2)
        + (df["mf_desc"].astype(int) * 1)
        + (df["mx_keys"].astype(int) * 1)
        - (df["penalty"].astype(int) * 1)
    )

    # è»½ã„ãƒ†ãƒ¼ãƒÃ—æ°—æŒã¡ ãƒœãƒ¼ãƒŠã‚¹ï¼ˆ+1ï¼‰
    BONUS_MAP = {
        ("ä»•äº‹ãƒ»ã‚­ãƒ£ãƒªã‚¢", "å°ã•ãå‹•ãå‡ºã—ãŸã„"),
        ("è‡ªå·±ç†è§£ãƒ»å†…çœ", "è‡ªåˆ†ã®è»¸ã‚’ç¢ºã‹ã‚ãŸã„"),
        ("äººé–“é–¢ä¿‚ãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "äººé–“é–¢ä¿‚ã‚’æ•´ãˆãŸã„"),
        ("æ‹æ„›ãƒ»ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—", "äººé–“é–¢ä¿‚ã‚’æ•´ãˆãŸã„"),
        ("ç¿’æ…£ãƒ»ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«", "å‰å‘ãã«ãªã‚ŠãŸã„"),
        ("ç¿’æ…£ãƒ»ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«", "å°ã•ãå‹•ãå‡ºã—ãŸã„"),
        ("æ­»ç”Ÿè¦³ãƒ»äººç”Ÿã®æ„å‘³", "è¿·ã„ã‚’æ•´ç†ã—ãŸã„"),
    }
    if (interest_choice, feeling_choice) in BONUS_MAP:
        df["score"] = df["score"] + 1

    # Tiered selection based on weighted scores
    strong = df[df["score"] >= 4]
    medium = df[(df["score"] >= 2) & (df["score"] < 4)]
    if len(strong) >= 3:
        return strong
    elif len(strong) + len(medium) >= 3:
        return pd.concat([strong, medium]).head(30)
    else:
        # æœ€å¾Œã®æ‰‹ã¨ã—ã¦ combined_kw ã®ã©ã‚Œã‹ã«ãƒãƒƒãƒ
        loose = df[df["_keys"].apply(lambda s: _match_any_keyword(s, combined_kw)) | df["_desc"].apply(lambda s: _match_any_keyword(s, combined_kw))]
        merged = pd.concat([strong, medium, loose]).drop_duplicates()
        if len(merged) >= 3:
            return merged.head(30)
        # ãã‚Œã§ã‚‚è¶³ã‚Šãªã‘ã‚Œã°å…¨ä½“
        return df

if st.button("ğŸ“– æœ¬ã‚’ãˆã‚‰ã¶"):
    candidates = filter_books(books, interest, feeling, extra)

    if len(candidates) == 0:
        st.info("æ¡ä»¶ã«ã´ã£ãŸã‚Šã®æœ¬ãŒå°‘ãªã‹ã£ãŸãŸã‚ã€å…¨ä½“ã‹ã‚‰ã‚‚ãŠã™ã™ã‚ã‚’é¸ã³ã¾ã—ãŸã€‚")

    # ã‚¹ã‚³ã‚¢é«˜ã„é †ã«ä¸¦ã¹ã€è¶³ã‚Šãªã‘ã‚Œã°å…¨ä½“ã‹ã‚‰è£œå®Œï¼ˆè£œå®Œå´ã‚‚ã‚¹ã‚³ã‚¢å„ªå…ˆï¼‰
    cols_for_sort = [c for c in ["score"] if c in candidates.columns]
    if len(candidates) >= 1 and cols_for_sort:
        candidates["_rand"] = np.random.rand(len(candidates))
        cand_sorted = candidates.sort_values(["score", "_rand"], ascending=[False, True])
    else:
        cand_sorted = candidates

    if len(cand_sorted) >= 3:
        # Prefer items that clearly match the selected theme
        theme_regex = "|".join(INTEREST_TO_KEYWORDS.get(interest, []))
        theme_mask_sorted = cand_sorted["_keys"].str.contains(theme_regex, case=False, na=False) | \
                            cand_sorted["_title"].str.contains(theme_regex, case=False, na=False)
        preferred = cand_sorted[theme_mask_sorted]
        if len(preferred) >= 3:
            picks = preferred.head(3)
        else:
            need = 3 - len(preferred)
            others = cand_sorted[~theme_mask_sorted]
            picks = pd.concat([preferred, others.head(need)], ignore_index=True)
    elif len(cand_sorted) >= 1:
        need = 3 - len(cand_sorted)
        # è£œå®Œå€™è£œï¼šæ—¢ã«é¸ã‚“ã ã‚‚ã®ã‚’é™¤ãã€åŒã˜ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ä¸€æ™‚çµåˆ
        rest = books.drop(cand_sorted.index, errors="ignore").copy()
        if "keywords" in rest.columns:
            # Recompute weighted scoring for rest, matching filter_books
            rest["_title"] = rest["title"].astype(str).str.lower()
            rest["_desc"]  = rest["description"].astype(str).str.lower()
            rest["_keys"]  = rest["keywords"].astype(str).str.lower()
            rest["mi_keys"] = rest["_keys"].apply(lambda s: _match_any_keyword(s, INTEREST_TO_KEYWORDS.get(interest, [])))
            rest["mi_title"] = rest["_title"].apply(lambda s: _match_any_keyword(s, INTEREST_TO_KEYWORDS.get(interest, [])))
            rest["mi_desc"] = rest["_desc"].apply(lambda s: _match_any_keyword(s, INTEREST_TO_KEYWORDS.get(interest, [])))
            rest["mf_keys"] = rest["_keys"].apply(lambda s: _match_any_keyword(s, FEELING_TO_KEYWORDS.get(feeling, [])))
            rest["mf_desc"] = rest["_desc"].apply(lambda s: _match_any_keyword(s, FEELING_TO_KEYWORDS.get(feeling, [])))
            rest["mx_keys"] = rest["_keys"].apply(lambda s: _match_any_keyword(s, EXTRA_TO_KEYWORDS.get(extra, [])))
            rest["penalty"] = rest["_keys"].apply(lambda s: _match_any_keyword(s, INTEREST_PENALTY.get(interest, [])))
            rest["score"] = (
                (rest["mi_keys"].astype(int) * 3)
                + (rest["mi_title"].astype(int) * 2)
                + (rest["mi_desc"].astype(int) * 1)
                + (rest["mf_keys"].astype(int) * 2)
                + (rest["mf_desc"].astype(int) * 1)
                + (rest["mx_keys"].astype(int) * 1)
                - (rest["penalty"].astype(int) * 1)
            )
            if (interest, feeling) in {
                ("ä»•äº‹ãƒ»ã‚­ãƒ£ãƒªã‚¢", "å°ã•ãå‹•ãå‡ºã—ãŸã„"),
                ("è‡ªå·±ç†è§£ãƒ»å†…çœ", "è‡ªåˆ†ã®è»¸ã‚’ç¢ºã‹ã‚ãŸã„"),
                ("äººé–“é–¢ä¿‚ãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "äººé–“é–¢ä¿‚ã‚’æ•´ãˆãŸã„"),
                ("æ‹æ„›ãƒ»ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—", "äººé–“é–¢ä¿‚ã‚’æ•´ãˆãŸã„"),
                ("ç¿’æ…£ãƒ»ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«", "å‰å‘ãã«ãªã‚ŠãŸã„"),
                ("ç¿’æ…£ãƒ»ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«", "å°ã•ãå‹•ãå‡ºã—ãŸã„"),
                ("æ­»ç”Ÿè¦³ãƒ»äººç”Ÿã®æ„å‘³", "è¿·ã„ã‚’æ•´ç†ã—ãŸã„"),
            }:
                rest["score"] = rest["score"] + 1
            rest["_rand"] = np.random.rand(len(rest))
            rest = rest.sort_values(["score", "_rand"], ascending=[False, True])
        picks = pd.concat([cand_sorted, rest.head(need)], ignore_index=True)
        # Re-order picks to prefer theme matches
        theme_regex = "|".join(INTEREST_TO_KEYWORDS.get(interest, []))
        tmask = picks["_keys"].str.contains(theme_regex, case=False, na=False) | \
                picks["_title"].str.contains(theme_regex, case=False, na=False)
        if tmask.any():
            picks = pd.concat([picks[tmask], picks[~tmask]]).head(3)
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…¨ä½“ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ 
        picks = books.sample(3)
        theme_regex = "|".join(INTEREST_TO_KEYWORDS.get(interest, []))
        picks["_title"] = picks["title"].astype(str).str.lower()
        picks["_keys"]  = picks["keywords"].astype(str).str.lower()
        tmask = picks["_keys"].str.contains(theme_regex, case=False, na=False) | \
                picks["_title"].str.contains(theme_regex, case=False, na=False)
        if tmask.any():
            picks = pd.concat([picks[tmask], picks[~tmask]]).head(3)

    # ä¸è¦ãª_randåˆ—ã‚’å‰Šé™¤
    if "_rand" in picks.columns:
        picks = picks.drop(columns=["_rand"])
    if "_rand" in candidates.columns:
        candidates = candidates.drop(columns=["_rand"])
    if 'rest' in locals() and "_rand" in rest.columns:
        rest = rest.drop(columns=["_rand"])

    # st.success("ãŠã™ã™ã‚ã®æœ¬ã¯ã“ã¡ã‚‰ã§ã™ï¼")
    st.markdown("## ğŸŒŸ ç‰¹ã«ãŠã™ã™ã‚ã®1å†Š")
    st.markdown("<div class='divider'></div>", unsafe_allow_html=True)

    # æœ€å„ªå…ˆã®1å†Šï¼ˆã‚«ãƒ¼ãƒ‰è¡¨ç¤ºï¼‰
    pick = picks.iloc[0]
    esc_title = html.escape(str(pick["title"]))
    esc_desc = html.escape(str(pick["description"]))
    hero_link = build_amazon_link(pick['title'], guess_author_from_keywords(pick.get('keywords', '')))
    hero_html = f"""
<div class="hero-card">
  <div class="hero-title">ã€{esc_title}ã€</div>
  <p class="hero-desc">{esc_desc}</p>
  <a class="link-btn" href="{hero_link}" target="_blank" rel="noopener" aria-label="Amazonã§{esc_title}ã‚’æ¤œç´¢">ğŸ“¦ Amazonã§è¦‹ã‚‹</a>
</div>
"""
    st.markdown("<div class='section section-hero'>" + hero_html + "</div>", unsafe_allow_html=True)

    st.markdown("## ğŸ“– ã“ã¡ã‚‰ã‚‚æ‰‹ã«ã¨ã£ã¦ã¿ã¾ã›ã‚“ã‹")
    st.markdown("<div class='divider'></div>", unsafe_allow_html=True)
    # æ¬¡ç‚¹ã®2å†Šï¼ˆã‚°ãƒªãƒƒãƒ‰ã§æ¨ªä¸¦ã³ï¼ã‚¹ãƒãƒ›ã¯ç¸¦ï¼‰
    cards_html = []
    for _, book in picks.iloc[1:].iterrows():
        esc_t = html.escape(str(book["title"]))
        esc_d = html.escape(str(book["description"]))
        link = build_amazon_link(book['title'], guess_author_from_keywords(book.get('keywords', '')))
        card_html = f"""
<div class="book-card">
  <div class="book-title">ã€{esc_t}ã€</div>
  <p class="book-desc">{esc_d}</p>
  <a class="link-btn" href="{link}" target="_blank" rel="noopener" aria-label="Amazonã§{esc_t}ã‚’æ¤œç´¢">ğŸ“¦ Amazonã§è¦‹ã‚‹</a>
</div>
"""
        cards_html.append(card_html)
    st.markdown("<div class='section section-others'><div class='book-grid'>" + "".join(cards_html) + "</div></div>", unsafe_allow_html=True)